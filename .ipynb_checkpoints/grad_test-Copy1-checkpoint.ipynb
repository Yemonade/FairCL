{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('D:\\\\Compute Science\\\\Machine Learning\\\\论文\\\\项目\\\\FairSPL\\\\venv_torch')\n",
    "sys.path.append('D:\\\\Compute Science\\\\Machine Learning\\\\论文\\\\项目\\\\FairSPL\\\\venv_torch\\\\lib\\\\site-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个简单的模型\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 3)\n",
    "        self.fc2 = nn.Linear(3, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "torch.Size([3])\n",
      "torch.Size([2, 3])\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "# 输出模型的参数\n",
    "model = MLP()\n",
    "for param in model.parameters():\n",
    "    print(param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义数据\n",
    "data = torch.tensor([1,2,3,4], dtype=torch.float)\n",
    "label = torch.tensor([5,6], dtype=torch.float)\n",
    "pred = model(data)\n",
    "loss_fn = nn.MSELoss()\n",
    "loss = loss_fn(pred, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算雅克比矩阵（一阶导数）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([23])\n"
     ]
    }
   ],
   "source": [
    "def grad(model, y):\n",
    "    \"\"\" 计算一阶导数.\n",
    "    Returns: \n",
    "        grads, grads[i]: dy / dx_i\n",
    "    \"\"\"\n",
    "    grads = torch.autograd.grad(loss, model.parameters(), retain_graph=True, create_graph=True)\n",
    "    # for grad in grads:\n",
    "    #     print(grad.size()) # 可以发现一共 4 个 Tensor，分别为损失函数对四个参数 Tensor（两层，每层都有权重和偏置）的梯度。\n",
    "        \n",
    "    grads = torch.cat([x.flatten() for x in grads], dim=0)\n",
    "    return grads\n",
    "\n",
    "grads = grad(model, loss)\n",
    "print(grads.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算 Hessian 矩阵（二阶导数）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([23, 23])\n"
     ]
    }
   ],
   "source": [
    "# 如果直接传入 model.parameters()，会报错，目前不知道原因\n",
    "def hess(model, y, grads=None):\n",
    "    \"\"\" 计算二阶导数.\n",
    "    Returns: \n",
    "        he, he[i,j]: d^2y / (dx_i dx_j)\n",
    "    \"\"\"\n",
    "    if grads is None:\n",
    "        grads = grad(model, y)\n",
    "        \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    he = torch.zeros(total_params, total_params)\n",
    "    \n",
    "    for i, g in enumerate(grads):\n",
    "        second_order_grad = grad(model, g)\n",
    "        he[i, :] = second_order_grad\n",
    "\n",
    "    return he\n",
    "\n",
    "he = hess(model, loss)\n",
    "print(he.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 计算 hessian 矩阵\n",
    "# grad = torch.autograd.grad(outputs=loss, inputs=model.parameters(), create_graph=True)\n",
    "# grad = torch.cat([x.flatten() for x in grad], dim=0)\n",
    "# total_params = sum(p.numel() for p in model.parameters())\n",
    "# he2 = torch.zeros(total_params, total_params)\n",
    "    \n",
    "# for i, g in enumerate(grad):\n",
    "#     second_order_grad = torch.autograd.grad(outputs=g, inputs=model.parameters(), retain_graph=True)\n",
    "#     second_order_grad = torch.cat([x.flatten() for x in second_order_grad], dim=0)\n",
    "#     he2[i, :] = second_order_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现影响函数（一次性返回所有点的影响函数值）\n",
    "$$\n",
    "\\mathcal{I}_{\\text {up,loss }}\\left(z, z_{\\text {test }}\\right) =-\\nabla_{\\theta} L\\left(z_{\\text {test }}, \\hat{\\theta}\\right)^{\\top} H_{\\hat{\\theta}}^{-1} \\nabla_{\\theta} L(z, \\hat{\\theta})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据准备\n",
    "train_set = torch.tensor([[1,2,3,4],[5,6,7,8]], dtype=torch.float)\n",
    "train_label = torch.tensor([[9,10],[11,12]], dtype=torch.float)\n",
    "test_point = torch.tensor([5,6,7,7], dtype=torch.float)\n",
    "test_label = torch.tensor([7,8], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step1: 计算V第三项\n",
    "def grad_z(z, t):\n",
    "    \"\"\" Calculates the gradient z. One grad_z should be computed for each\n",
    "    training sample.\n",
    "    \n",
    "    Arguments:\n",
    "        z: torch tensor, training data points\n",
    "            e.g. an image sample (batch_size, 3, 256, 256)\n",
    "        t: torch tensor, training data labels\n",
    "        model: torch NN, model used to evaluate the dataset\n",
    "    Returns:\n",
    "        grad_z: list of torch tensor, containing the gradients\n",
    "            from model parameters to loss\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    y = model(z)\n",
    "    loss = loss_fn(y, t)\n",
    "    params = [ p for p in model.parameters() if p.requires_grad ]\n",
    "    grads = list(torch.autograd.grad(loss, params, create_graph=True))\n",
    "    #for grad in grads:\n",
    "    #   print(grad.size()) # 可以发现一共 4 个 Tensor，分别为损失函数对四个参数 Tensor（两层，每层都有权重和偏置）的梯度。\n",
    "        \n",
    "    return grads\n",
    "\n",
    "grad = grad_z(train_set, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 验证是否是每一个样本的loss的梯度的和\n",
    "grad1, grad2 = [grad_z(z,t) for (z, t) in zip(train_set, train_label)]\n",
    "g1 = torch.cat([x.flatten() for x in grad], dim=0)\n",
    "g2 = torch.cat([x.flatten() for x in grad1], dim=0)\n",
    "g3 = torch.cat([x.flatten() for x in grad2], dim=0)\n",
    "g1.allclose((g2+g3)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step2: 计算前两项，也就是 s_test=v^T H^{-1}\n",
    "# 辅助函数：快速计算 Hv，其中 H 是 Hessian 矩阵\n",
    "def hvp(y, w, v): # 计算 y 对 w 的二阶导 H，返回 Hv\n",
    "    \"\"\"Multiply the Hessians of y and w by v.\n",
    "    Uses a backprop-like approach to compute the product between the Hessian\n",
    "    and another vector efficiently, which even works for large Hessians.\n",
    "    Example: if: y = 0.5 * w^T A x then hvp(y, w, v) returns and expression\n",
    "    which evaluates to the same values as (A + A.t) v.\n",
    "\n",
    "    Arguments:\n",
    "        y: scalar/tensor, for example the output of the loss function\n",
    "        w: list of torch tensors, tensors over which the Hessian\n",
    "            should be constructed\n",
    "        v: list of torch tensors, same shape as w,\n",
    "            will be multiplied with the Hessian\n",
    "\n",
    "    Returns:\n",
    "        return_grads: list of torch tensors, contains product of Hessian and v.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: `y` and `w` have a different length.\"\"\"\n",
    "    if len(w) != len(v):\n",
    "        raise(ValueError(\"w and v must have the same length.\"))\n",
    "\n",
    "    # First backprop\n",
    "    first_grads = grad(y, w, retain_graph=True, create_graph=True)\n",
    "\n",
    "    # Elementwise products\n",
    "    elemwise_products = 0\n",
    "    for grad_elem, v_elem in zip(first_grads, v):\n",
    "        elemwise_products += torch.sum(grad_elem * v_elem)\n",
    "\n",
    "    # Second backprop\n",
    "    return_grads = grad(elemwise_products, w, create_graph=True)\n",
    "\n",
    "    return return_grads\n",
    "\n",
    "\n",
    "def s_test(z_test, t_test, model, z_loader, damp=0.01, scale=25.0,\n",
    "       recursion_depth=5000):\n",
    "    \"\"\"s_test can be precomputed for each test point of interest, and then\n",
    "    multiplied with grad_z to get the desired value for each training point.\n",
    "    Here, strochastic estimation is used to calculate s_test. s_test is the\n",
    "    Inverse Hessian Vector Product.\n",
    "\n",
    "    Arguments:\n",
    "        z_test: torch tensor, test data points, such as test images\n",
    "        t_test: torch tensor, contains all test data labels\n",
    "        model: torch NN, model used to evaluate the dataset\n",
    "        z_loader: torch Dataloader, can load the training dataset\n",
    "        damp: float, dampening factor\n",
    "        scale: float, scaling factor\n",
    "        recursion_depth: int, number of iterations aka recursion depth\n",
    "            should be enough so that the value stabilises.\n",
    "\n",
    "    Returns:\n",
    "        h_estimate: list of torch tensors, s_test\n",
    "    \"\"\"\n",
    "    v = grad_z(z_test, y_test, model)\n",
    "    h_estimate = v.copy()\n",
    "    for i in range(recursion_depth):\n",
    "        for x, t in z_loader():\n",
    "            y = model(x)\n",
    "            loss = calc_loss(y, t)\n",
    "            params = [ p for p in model.parameters() if p.requires_grad ]\n",
    "            hv = hvp(loss, params, h_estimate) \n",
    "            h_estimate = [\n",
    "                _v + (1 - damp) * _h_e - _hv / scale\n",
    "                for _v, _h_e, _hv in zip(v, h_estimate, hv)]\n",
    "            break\n",
    "    return h_estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22476\\4260895049.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ms_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_point\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22476\\2914924556.py\u001b[0m in \u001b[0;36ms_test\u001b[1;34m(z_test, t_test, model, z_loader, damp, scale, recursion_depth)\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[0mh_estimate\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms_test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \"\"\"\n\u001b[1;32m---> 59\u001b[1;33m     \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrad_z\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m     \u001b[0mh_estimate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecursion_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "s_test(test_point, test_label, model, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step3: 合并所有的结果\n",
    "def calc_influence_function(train_dataset_size, grad_z_vecs, e_s_test):\n",
    "    \"\"\"Calculates the influence function\n",
    "\n",
    "    Arguments:\n",
    "        train_dataset_size: int, total train dataset size\n",
    "        grad_z_vecs: list of torch tensor, containing the gradients\n",
    "            from model parameters to loss\n",
    "        e_s_test: list of torch tensor, contains s_test vectors\n",
    "\n",
    "    Returns:\n",
    "        influence: list of float, influences of all training data samples\n",
    "            for one test sample\n",
    "        harmful: list of float, influences sorted by harmfulness\n",
    "        helpful: list of float, influences sorted by helpfulness.\n",
    "    \"\"\"\n",
    "    influences = []\n",
    "    # 对第 i 个样本\n",
    "    for i in range(train_dataset_size):\n",
    "        tmp_influence = -sum(\n",
    "            [\n",
    "                torch.sum(k * j).data.cpu().numpy()\n",
    "                for k, j in zip(grad_z_vecs[i], e_s_test)\n",
    "            ]) / train_dataset_size\n",
    "        influences.append(tmp_influence)\n",
    "\n",
    "    harmful = np.argsort(influences)\n",
    "    helpful = harmful[::-1]\n",
    "\n",
    "    return influences, harmful.tolist(), helpful.tolist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
