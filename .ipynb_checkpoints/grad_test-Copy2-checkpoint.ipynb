{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('D:\\\\Compute Science\\\\Machine Learning\\\\论文\\\\项目\\\\FairSPL\\\\venv_torch')\n",
    "sys.path.append('D:\\\\Compute Science\\\\Machine Learning\\\\论文\\\\项目\\\\FairSPL\\\\venv_torch\\\\lib\\\\site-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个简单的模型\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 3)\n",
    "        self.fc2 = nn.Linear(3, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "torch.Size([3])\n",
      "torch.Size([2, 3])\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "# 输出模型的参数\n",
    "model = MLP()\n",
    "for param in model.parameters():\n",
    "    print(param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义数据\n",
    "data = torch.tensor([1,2,3,4], dtype=torch.float)\n",
    "label = torch.tensor([5,6], dtype=torch.float)\n",
    "pred = model(data)\n",
    "loss = loss_fn(pred, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算雅克比矩阵（一阶导数）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([23])\n"
     ]
    }
   ],
   "source": [
    "def grad(model, y):\n",
    "    \"\"\" 计算一阶导数.\n",
    "    Returns: \n",
    "        grads, grads[i]: dy / dx_i\n",
    "    \"\"\"\n",
    "    grads = torch.autograd.grad(loss, model.parameters(), retain_graph=True, create_graph=True)\n",
    "    # for grad in grads:\n",
    "    #     print(grad.size()) # 可以发现一共 4 个 Tensor，分别为损失函数对四个参数 Tensor（两层，每层都有权重和偏置）的梯度。\n",
    "        \n",
    "    grads = torch.cat([x.flatten() for x in grads], dim=0)\n",
    "    return grads\n",
    "\n",
    "grads = grad(model, loss)\n",
    "print(grads.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算 Hessian 矩阵（二阶导数）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([23, 23])\n"
     ]
    }
   ],
   "source": [
    "# 如果直接传入 model.parameters()，会报错，目前不知道原因\n",
    "def hess(model, y, grads=None):\n",
    "    \"\"\" 计算二阶导数.\n",
    "    Returns: \n",
    "        he, he[i,j]: d^2y / (dx_i dx_j)\n",
    "    \"\"\"\n",
    "    if grads is None:\n",
    "        grads = grad(model, y)\n",
    "        \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    he = torch.zeros(total_params, total_params)\n",
    "    \n",
    "    for i, g in enumerate(grads):\n",
    "        second_order_grad = grad(model, g)\n",
    "        he[i, :] = second_order_grad\n",
    "\n",
    "    return he\n",
    "\n",
    "he = hess(model, loss)\n",
    "print(he.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 计算 hessian 矩阵\n",
    "# grad = torch.autograd.grad(outputs=loss, inputs=model.parameters(), create_graph=True)\n",
    "# grad = torch.cat([x.flatten() for x in grad], dim=0)\n",
    "# total_params = sum(p.numel() for p in model.parameters())\n",
    "# he2 = torch.zeros(total_params, total_params)\n",
    "    \n",
    "# for i, g in enumerate(grad):\n",
    "#     second_order_grad = torch.autograd.grad(outputs=g, inputs=model.parameters(), retain_graph=True)\n",
    "#     second_order_grad = torch.cat([x.flatten() for x in second_order_grad], dim=0)\n",
    "#     he2[i, :] = second_order_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现影响函数（一次性返回所有点的影响函数值）\n",
    "$$\n",
    "\\mathcal{I}_{\\text {up,loss }}\\left(z, z_{\\text {test }}\\right) =-\\nabla_{\\theta} L\\left(z_{\\text {test }}, \\hat{\\theta}\\right)^{\\top} H_{\\hat{\\theta}}^{-1} \\nabla_{\\theta} L(z, \\hat{\\theta})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据准备\n",
    "train_z = torch.tensor([[1,2,3,4],[5,6,7,8]], dtype=torch.float)\n",
    "train_t = torch.tensor([[9,10],[11,12]], dtype=torch.float)\n",
    "\n",
    "train_set = [(z, t) for z, t in zip(train_z, train_t)]\n",
    "\n",
    "test_z = torch.tensor([5,6,7,7], dtype=torch.float)\n",
    "test_t = torch.tensor([7,8], dtype=torch.float)\n",
    "\n",
    "model = MLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  5.8607,   7.7541,   9.6475,  11.5409,  -3.7036,  -5.1888,  -6.6740,\n",
       "         -8.1592,  22.7662,  30.3566,  37.9470,  45.5374,   1.8934,  -1.4852,\n",
       "          7.5904,  19.9988, -25.1778,  22.7012,  26.1847, -33.6491,  29.9575,\n",
       "         -7.0601,  -8.8066], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step1: 计算V第三项\n",
    "def grad_z2(model, z, t):\n",
    "    \"\"\" Calculates the gradient z. One grad_z should be computed for each\n",
    "    training sample.\n",
    "    \n",
    "    Arguments:\n",
    "        z: torch tensor, training data points\n",
    "            e.g. an image sample (batch_size, 3, 256, 256)\n",
    "        t: torch tensor, training data labels\n",
    "        model: torch NN, model used to evaluate the dataset\n",
    "    Returns:\n",
    "        grad_z: list of torch tensor, containing the gradients\n",
    "            from model parameters to loss\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    y = model(z)\n",
    "    loss = loss_fn(y, t)\n",
    "    grads = torch.autograd.grad(loss, model.parameters(), create_graph=True)\n",
    "    grads = torch.cat([x.flatten() for x in grads], dim=0)\n",
    "\n",
    "    return grads\n",
    "\n",
    "grad = grad_z(model, train_z, train_t)\n",
    "grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 验证是否是每一个样本的loss的梯度的和的均值等于在batch上loss的计算结果\n",
    "grad1, grad2 = [grad_z(model, z, t) for (z, t) in train_set]\n",
    "grad.allclose((grad1+grad2)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step2: 计算前两项，也就是 s_test=v^T H^{-1}\n",
    "# 辅助函数：快速计算 Hv，其中 H 是 Hessian 矩阵\n",
    "def hvp(y, model, v): # 计算 y 对 w 的二阶导 H，返回 Hv\n",
    "    \"\"\"Multiply the Hessians of y and w by v.\n",
    "    Uses a backprop-like approach to compute the product between the Hessian\n",
    "    and another vector efficiently, which even works for large Hessians.\n",
    "    Example: if: y = 0.5 * w^T A x then hvp(y, w, v) returns and expression\n",
    "    which evaluates to the same values as (A + A.t) v.\n",
    "\n",
    "    Arguments:\n",
    "        y: scalar/tensor, for example the output of the loss function\n",
    "        w: list of torch tensors, tensors over which the Hessian\n",
    "            should be constructed\n",
    "        v: list of torch tensors, same shape as w,\n",
    "            will be multiplied with the Hessian\n",
    "\n",
    "    Returns:\n",
    "        return_grads: list of torch tensors, contains product of Hessian and v.\n",
    "\n",
    "    \"\"\"\n",
    "    # First backprop\n",
    "    first_grads = torch.autograd.grad(y, model.parameters(), retain_graph=True, create_graph=True)\n",
    "\n",
    "    # Elementwise products\n",
    "    elemwise_products = 0\n",
    "    for grad_elem, v_elem in zip(first_grads, v):\n",
    "        elemwise_products += torch.sum(grad_elem * v_elem)\n",
    "\n",
    "    # Second backprop\n",
    "    return_grads = torch.autograd.grad(elemwise_products, model.parameters(), create_graph=True)\n",
    "\n",
    "    return return_grads\n",
    "\n",
    "\n",
    "def s_test(model, test_z, test_t, train_loader, damp=0.01, scale=25.0,\n",
    "       recursion_depth=5000):\n",
    "    \"\"\"s_test can be precomputed for each test point of interest, and then\n",
    "    multiplied with grad_z to get the desired value for each training point.\n",
    "    Here, strochastic estimation is used to calculate s_test. s_test is the\n",
    "    Inverse Hessian Vector Product.\n",
    "\n",
    "    Arguments:\n",
    "        test_z: torch tensor, test data points, such as test images\n",
    "        test_t: torch tensor, contains all test data labels\n",
    "        model: torch NN, model used to evaluate the dataset\n",
    "        train_loader: torch Dataloader, can load the training dataset\n",
    "        damp: float, dampening factor\n",
    "        scale: float, scaling factor\n",
    "        recursion_depth: int, number of iterations aka recursion depth\n",
    "            should be enough so that the value stabilises.\n",
    "\n",
    "    Returns:\n",
    "        h_estimate: list of torch tensors, s_test\n",
    "    \"\"\"\n",
    "    v = grad_z(model, test_z, test_t)\n",
    "    h_estimate = v.clone().detach()\n",
    "    for i in range(recursion_depth):\n",
    "        for z, t in train_loader:\n",
    "            y = model(z)\n",
    "            loss = loss_fn(y, t)\n",
    "            hv = hvp(loss, model, h_estimate) \n",
    "            h_estimate = [\n",
    "                _v + (1 - damp) * _h_e - _hv / scale\n",
    "                for _v, _h_e, _hv in zip(v, h_estimate, hv)]\n",
    "            break\n",
    "    return h_estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[16.5433, 21.2170, 25.8908, 30.5645],\n",
       "         [16.4665, 21.0635, 25.6605, 30.2575],\n",
       "         [15.5208, 19.1720, 22.8233, 26.4746]], grad_fn=<SubBackward0>),\n",
       " tensor([18.9172, 18.8404, 17.8947], grad_fn=<SubBackward0>),\n",
       " tensor([[37.2072, 36.8563, 37.2114],\n",
       "         [33.7050, 44.2962, 33.5768]], grad_fn=<SubBackward0>),\n",
       " tensor([16.5154, 19.6926], grad_fn=<SubBackward0>)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_test(model, test_z, test_t, train_set, recursion_depth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3590572"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.7720*0.4651"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step3: 合并所有的结果 计算每一个样本点对test_z的影响值\n",
    "def calc_influence_function(model, train_z, train_t, test_z, test_t):\n",
    "    \"\"\"Calculates the influence function\n",
    "\n",
    "    Arguments:\n",
    "        grad_z_vecs: list of torch tensor, containing the gradients\n",
    "            from model parameters to loss\n",
    "        e_s_test: list of torch tensor, contains s_test vectors\n",
    "\n",
    "    Returns:\n",
    "        influence: list of float, influences of all training data samples\n",
    "            for one test sample\n",
    "        harmful: list of float, influences sorted by harmfulness\n",
    "        helpful: list of float, influences sorted by helpfulness.\n",
    "    \"\"\"\n",
    "    train_dataset_size = len(train_z)\n",
    "    influences = []\n",
    "    \n",
    "    grad_z_vecs = [grad_z(model, z, t) for z, t in zip(train_z, train_t)] \n",
    "    e_s_test = s_test(model, test_z, test_t, zip(train_z, train_t))\n",
    "    \n",
    "#     grad_z_vecs = torch.cat([x.flatten() for x in grad_z_vecs], dim=0)\n",
    "#     e_s_test = torch.cat([x.flatten() for x in e_s_test], dim=0)\n",
    "    \n",
    "    # 对第 i 个样本\n",
    "    for i in range(train_dataset_size):\n",
    "        influence_i = -sum(\n",
    "            [\n",
    "                torch.sum(k * j).data.cpu().numpy()\n",
    "                for k, j in zip(grad_z_vecs[i], e_s_test)\n",
    "            ]) / train_dataset_size\n",
    "        influences.append(influence_i)\n",
    "\n",
    "    harmful = np.argsort(influences)\n",
    "    helpful = harmful[::-1]\n",
    "\n",
    "    return influences, harmful.tolist(), helpful.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_influence_function(model, train_x, train_label, test_z, test_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  5.8607,   7.7541,   9.6475,  11.5409,  -3.7036,  -5.1888,  -6.6740,\n",
       "         -8.1592,  22.7662,  30.3566,  37.9470,  45.5374,   1.8934,  -1.4852,\n",
       "          7.5904,  19.9988, -25.1778,  22.7012,  26.1847, -33.6491,  29.9575,\n",
       "         -7.0601,  -8.8066], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step1: 计算V第三项\n",
    "def grad_z2(model, z, t):\n",
    "    \"\"\" Calculates the gradient z. One grad_z should be computed for each\n",
    "    training sample.\n",
    "    \n",
    "    Arguments:\n",
    "        z: torch tensor, training data points\n",
    "            e.g. an image sample (batch_size, 3, 256, 256)\n",
    "        t: torch tensor, training data labels\n",
    "        model: torch NN, model used to evaluate the dataset\n",
    "    Returns:\n",
    "        grad_z: list of torch tensor, containing the gradients\n",
    "            from model parameters to loss\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    y = model(z)\n",
    "    loss = loss_fn(y, t)\n",
    "    grads = torch.autograd.grad(loss, model.parameters(), create_graph=True)\n",
    "    grads = torch.cat([x.flatten() for x in grads], dim=0)\n",
    "\n",
    "    return grads\n",
    "\n",
    "grad = grad_z2(model, train_z, train_t)\n",
    "grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step1: 计算V第三项\n",
    "def grad_z2(model, z, t):\n",
    "    \"\"\" Calculates the gradient z. One grad_z should be computed for each\n",
    "    training sample.\n",
    "    \n",
    "    Arguments:\n",
    "        z: torch tensor, training data points\n",
    "            e.g. an image sample (batch_size, 3, 256, 256)\n",
    "        t: torch tensor, training data labels\n",
    "        model: torch NN, model used to evaluate the dataset\n",
    "    Returns:\n",
    "        grad_z: list of torch tensor, containing the gradients\n",
    "            from model parameters to loss\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    y = model(z)\n",
    "    loss = loss_fn(y, t)\n",
    "    grads = torch.autograd.grad(loss, model.parameters(), create_graph=True)\n",
    "    grads = torch.cat([x.flatten() for x in grads], dim=0)\n",
    "\n",
    "    return grads\n",
    "\n",
    "# grad = grad_z2(model, train_z, train_t)\n",
    "# grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step2: 计算前两项，也就是 s_test=v^T H^{-1}\n",
    "# 辅助函数：快速计算 Hv，其中 H 是 Hessian 矩阵\n",
    "def hvp2(y, model, v): # 计算 y 对 w 的二阶导 H，返回 Hv\n",
    "    \"\"\"Multiply the Hessians of y and w by v.\n",
    "    Uses a backprop-like approach to compute the product between the Hessian\n",
    "    and another vector efficiently, which even works for large Hessians.\n",
    "    Example: if: y = 0.5 * w^T A x then hvp(y, w, v) returns and expression\n",
    "    which evaluates to the same values as (A + A.t) v.\n",
    "\n",
    "    Arguments:\n",
    "        y: scalar/tensor, for example the output of the loss function\n",
    "        w: list of torch tensors, tensors over which the Hessian\n",
    "            should be constructed\n",
    "        v: list of torch tensors, same shape as w,\n",
    "            will be multiplied with the Hessian\n",
    "\n",
    "    Returns:\n",
    "        return_grads: list of torch tensors, contains product of Hessian and v.\n",
    "\n",
    "    \"\"\"\n",
    "    # First backprop\n",
    "    first_grads = torch.autograd.grad(y, model.parameters(), retain_graph=True, create_graph=True)\n",
    "    first_grads = torch.cat([x.flatten() for x in first_grads], dim=0)\n",
    "    # Elementwise products\n",
    "    elemwise_products = 0\n",
    "    for grad_elem, v_elem in zip(first_grads, v):\n",
    "        elemwise_products += torch.sum(grad_elem * v_elem)\n",
    "    \n",
    "    # Second backprop\n",
    "    return_grads = torch.autograd.grad(elemwise_products, model.parameters(), create_graph=True)\n",
    "\n",
    "    return return_grads\n",
    "\n",
    "\n",
    "def s_test2(model, test_z, test_t, train_loader, damp=0.01, scale=25.0,\n",
    "       recursion_depth=5000):\n",
    "    \"\"\"s_test can be precomputed for each test point of interest, and then\n",
    "    multiplied with grad_z to get the desired value for each training point.\n",
    "    Here, strochastic estimation is used to calculate s_test. s_test is the\n",
    "    Inverse Hessian Vector Product.\n",
    "\n",
    "    Arguments:\n",
    "        test_z: torch tensor, test data points, such as test images\n",
    "        test_t: torch tensor, contains all test data labels\n",
    "        model: torch NN, model used to evaluate the dataset\n",
    "        train_loader: torch Dataloader, can load the training dataset\n",
    "        damp: float, dampening factor\n",
    "        scale: float, scaling factor\n",
    "        recursion_depth: int, number of iterations aka recursion depth\n",
    "            should be enough so that the value stabilises.\n",
    "\n",
    "    Returns:\n",
    "        h_estimate: list of torch tensors, s_test\n",
    "    \"\"\"\n",
    "    v = grad_z2(model, test_z, test_t)\n",
    "    h_estimate = v.clone().detach()\n",
    "    for i in range(recursion_depth):\n",
    "        for z, t in train_loader:\n",
    "            y = model(z)\n",
    "            loss = loss_fn(y, t)\n",
    "            hv = hvp2(loss, model, h_estimate) \n",
    "            h_estimate = [\n",
    "                _v + (1 - damp) * _h_e - _hv / scale\n",
    "                for _v, _h_e, _hv in zip(v, h_estimate, hv)]\n",
    "            break\n",
    "    return h_estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step3: 合并所有的结果 计算每一个样本点对test_z的影响值\n",
    "def calc_influence_function(model, train_set, test_z, test_t):\n",
    "    \"\"\"Calculates the influence function\n",
    "\n",
    "    Arguments:\n",
    "        grad_z_vecs: list of torch tensor, containing the gradients\n",
    "            from model parameters to loss\n",
    "        e_s_test: list of torch tensor, contains s_test vectors\n",
    "\n",
    "    Returns:\n",
    "        influence: list of float, influences of all training data samples\n",
    "            for one test sample\n",
    "        harmful: list of float, influences sorted by harmfulness\n",
    "        helpful: list of float, influences sorted by helpfulness.\n",
    "    \"\"\"\n",
    "    train_dataset_size = len(train_set)\n",
    "    influences = []\n",
    "    \n",
    "    grad_z_vecs = [grad_z2(model, z, t) for z, t in train_set] \n",
    "    e_s_test = s_test2(model, test_z, test_t, train_set, recursion_depth=1)\n",
    "    \n",
    "#     grad_z_vecs = torch.cat([x.flatten() for x in grad_z_vecs], dim=0)\n",
    "#     e_s_test = torch.cat([x.flatten() for x in e_s_test], dim=0)\n",
    "    \n",
    "    # 对第 i 个样本\n",
    "    for i in range(train_dataset_size):\n",
    "        influence_i = -sum(\n",
    "            [\n",
    "                torch.sum(k * j).data.cpu().numpy()\n",
    "                for k, j in zip(grad_z_vecs[i], e_s_test)\n",
    "            ]) / train_dataset_size\n",
    "        influences.append(influence_i)\n",
    "\n",
    "    harmful = np.argsort(influences)\n",
    "    helpful = harmful[::-1]\n",
    "\n",
    "    return influences, harmful.tolist(), helpful.tolist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
