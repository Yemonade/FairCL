{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('D:\\\\Compute Science\\\\Machine Learning\\\\论文\\\\项目\\\\FairSPL\\\\venv_torch')\n",
    "sys.path.append('D:\\\\Compute Science\\\\Machine Learning\\\\论文\\\\项目\\\\FairSPL\\\\venv_torch\\\\lib\\\\site-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个简单的模型\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 3)\n",
    "        self.fc2 = nn.Linear(3, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "torch.Size([3])\n",
      "torch.Size([2, 3])\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "# 输出模型的参数\n",
    "model = MLP()\n",
    "for param in model.parameters():\n",
    "    print(param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义数据\n",
    "data = torch.tensor([1,2,3,4], dtype=torch.float)\n",
    "label = torch.tensor([5,6], dtype=torch.float)\n",
    "pred = model(data)\n",
    "loss = loss_fn(pred, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算雅克比矩阵（一阶导数）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -1.8790,  -3.7581,  -5.6371,  -7.5161,   1.2390,   2.4781,   3.7171,\n",
      "          4.9561,  -1.3145,  -2.6290,  -3.9435,  -5.2579,  -1.8790,   1.2390,\n",
      "         -1.3145,   7.3481,  -4.0111,  -9.5360,   9.2882,  -5.0701, -12.0537,\n",
      "         -5.1941,  -6.5655], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def grad(model, y):\n",
    "    \"\"\" 计算一阶导数.\n",
    "    Returns: \n",
    "        grads, grads[i]: dy / dx_i\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    grads = torch.autograd.grad(loss, model.parameters(), retain_graph=True, create_graph=True)\n",
    "    # for grad in grads:\n",
    "    #     print(grad.size()) # 可以发现一共 4 个 Tensor，分别为损失函数对四个参数 Tensor（两层，每层都有权重和偏置）的梯度。\n",
    "        \n",
    "    grads = torch.cat([x.flatten() for x in grads], dim=0)\n",
    "    return grads\n",
    "\n",
    "grads = grad(model, loss)\n",
    "print(grads.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算 Hessian 矩阵（二阶导数）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([23, 23])\n"
     ]
    }
   ],
   "source": [
    "# 如果函数定义为 ``hess(w, y, ...)\n",
    "# 直接传入 model.parameters()，会报错，\n",
    "# 与grad_z唯一的不同是使用了两次 w（grad_z可以这么定义，直接传入model.parameters()）\n",
    "# 目前不知道原因\n",
    "def hess(model, y, grads=None):\n",
    "    \"\"\" 计算二阶导数.\n",
    "    Returns: \n",
    "        he, he[i,j]: d^2y / (dx_i dx_j)\n",
    "    \"\"\"\n",
    "    if grads is None:\n",
    "        grads = grad(model, y)\n",
    "       \n",
    "    model.eval()\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    he = torch.zeros(total_params, total_params)\n",
    "    \n",
    "    for i, g in enumerate(grads):\n",
    "        second_order_grad = grad(model, g)\n",
    "        he[i, :] = second_order_grad\n",
    "\n",
    "    return he\n",
    "\n",
    "he = hess(model, loss)\n",
    "print(he.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 计算 hessian 矩阵\n",
    "# grad = torch.autograd.grad(outputs=loss, inputs=model.parameters(), create_graph=True)\n",
    "# grad = torch.cat([x.flatten() for x in grad], dim=0)\n",
    "# total_params = sum(p.numel() for p in model.parameters())\n",
    "# he2 = torch.zeros(total_params, total_params)\n",
    "    \n",
    "# for i, g in enumerate(grad):\n",
    "#     second_order_grad = torch.autograd.grad(outputs=g, inputs=model.parameters(), retain_graph=True)\n",
    "#     second_order_grad = torch.cat([x.flatten() for x in second_order_grad], dim=0)\n",
    "#     he2[i, :] = second_order_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现影响函数（一次性返回所有点的影响函数值）\n",
    "$$\n",
    "\\mathcal{I}_{\\text {up,loss }}\\left(z, z_{\\text {test }}\\right) =-\\nabla_{\\theta} L\\left(z_{\\text {test }}, \\hat{\\theta}\\right)^{\\top} H_{\\hat{\\theta}}^{-1} \\nabla_{\\theta} L(z, \\hat{\\theta})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据准备\n",
    "train_z = torch.tensor([[1,2,3,4],[5,6,7,8]], dtype=torch.float)\n",
    "train_t = torch.tensor([[9,10],[11,12]], dtype=torch.float)\n",
    "\n",
    "train_set = [(z, t) for z, t in zip(train_z, train_t)]\n",
    "\n",
    "test_z = torch.tensor([5,6,7,7], dtype=torch.float)\n",
    "test_t = torch.tensor([7,8], dtype=torch.float)\n",
    "\n",
    "model = MLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 34.4961,  45.1405,  55.7848,  66.4292],\n",
       "         [  8.2716,  10.8364,  13.4013,  15.9661],\n",
       "         [-25.4239, -33.2688, -41.1137, -48.9586]], grad_fn=<TBackward0>),\n",
       " tensor([10.6444,  2.5649, -7.8449], grad_fn=<ViewBackward0>),\n",
       " tensor([[-15.6780,  23.3260,   7.7328],\n",
       "         [-18.5366,  27.5539,   9.1455]], grad_fn=<TBackward0>),\n",
       " tensor([-10.3758, -12.2205], grad_fn=<ViewBackward0>)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step1: 计算V第三项\n",
    "def grad_z(model, z, t):\n",
    "    \"\"\" Calculates the gradient z. One grad_z should be computed for each\n",
    "    training sample.\n",
    "    \n",
    "    Arguments:\n",
    "        z: torch tensor, training data points\n",
    "            e.g. an image sample (batch_size, 3, 256, 256)\n",
    "        t: torch tensor, training data labels\n",
    "        model: torch NN, model used to evaluate the dataset\n",
    "    Returns:\n",
    "        grad_z: list of torch tensor, containing the gradients\n",
    "            from model parameters to loss\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    y = model(z)\n",
    "    loss = loss_fn(y, t)\n",
    "    params = [ p for p in model.parameters() if p.requires_grad ]\n",
    "    grads = list(torch.autograd.grad(loss, params, create_graph=True))\n",
    "    \n",
    "    #for grad in grads:\n",
    "    #   print(grad.size()) # 可以发现一共 4 个 Tensor，分别为损失函数对四个参数 Tensor（两层，每层都有权重和偏置）的梯度。\n",
    "        \n",
    "    return grads\n",
    "\n",
    "grad = grad_z(model, train_z, train_t)\n",
    "grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 验证是否是每一个样本的loss的梯度的和的均值等于在batch上loss的计算结果\n",
    "grad1, grad2 = [grad_z(model, z, t) for (z, t) in train_set]\n",
    "g1 = torch.cat([x.flatten() for x in grad], dim=0)\n",
    "g2 = torch.cat([x.flatten() for x in grad1], dim=0)\n",
    "g3 = torch.cat([x.flatten() for x in grad2], dim=0)\n",
    "g1.allclose((g2+g3)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step2: 计算前两项，也就是 s_test=v^T H^{-1}\n",
    "# 辅助函数：快速计算 Hv，其中 H 是 Hessian 矩阵\n",
    "def hvp(y, w, v): # 计算 y 对 w 的二阶导 H，返回 Hv\n",
    "    \"\"\"Multiply the Hessians of y and w by v.\n",
    "    Uses a backprop-like approach to compute the product between the Hessian\n",
    "    and another vector efficiently, which even works for large Hessians.\n",
    "    Example: if: y = 0.5 * w^T A x then hvp(y, w, v) returns and expression\n",
    "    which evaluates to the same values as (A + A.t) v.\n",
    "\n",
    "    Arguments:\n",
    "        y: scalar/tensor, for example the output of the loss function\n",
    "        w: list of torch tensors, tensors over which the Hessian\n",
    "            should be constructed\n",
    "        v: list of torch tensors, same shape as w,\n",
    "            will be multiplied with the Hessian\n",
    "\n",
    "    Returns:\n",
    "        return_grads: list of torch tensors, contains product of Hessian and v.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: `y` and `w` have a different length.\"\"\"\n",
    "    if len(w) != len(v):\n",
    "        raise(ValueError(\"w and v must have the same length.\"))\n",
    "\n",
    "    # First backprop\n",
    "    first_grads = torch.autograd.grad(y, w, retain_graph=True, create_graph=True)\n",
    "    # Elementwise products\n",
    "    elemwise_products = 0\n",
    "    for grad_elem, v_elem in zip(first_grads, v):\n",
    "        elemwise_products += torch.sum(grad_elem * v_elem)\n",
    "\n",
    "\n",
    "    # Second backprop\n",
    "    return_grads = torch.autograd.grad(elemwise_products, w, create_graph=True)\n",
    "\n",
    "    return return_grads\n",
    "\n",
    "\n",
    "def s_test(model, test_z, test_t, train_loader, damp=0.01, scale=25.0,\n",
    "       recursion_depth=5000):\n",
    "    \"\"\"s_test can be precomputed for each test point of interest, and then\n",
    "    multiplied with grad_z to get the desired value for each training point.\n",
    "    Here, strochastic estimation is used to calculate s_test. s_test is the\n",
    "    Inverse Hessian Vector Product.\n",
    "\n",
    "    Arguments:\n",
    "        test_z: torch tensor, test data points, such as test images\n",
    "        test_t: torch tensor, contains all test data labels\n",
    "        model: torch NN, model used to evaluate the dataset\n",
    "        train_loader: torch Dataloader, can load the training dataset\n",
    "        damp: float, dampening factor\n",
    "        scale: float, scaling factor\n",
    "        recursion_depth: int, number of iterations aka recursion depth\n",
    "            should be enough so that the value stabilises.\n",
    "\n",
    "    Returns:\n",
    "        h_estimate: list of torch tensors, s_test\n",
    "    \"\"\"\n",
    "    v = grad_z(model, test_z, test_t)\n",
    "    h_estimate = v.copy()\n",
    "    for i in range(recursion_depth):\n",
    "        for z, t in train_loader:\n",
    "            y = model(z)\n",
    "            loss = loss_fn(y, t)\n",
    "            params = [ p for p in model.parameters() if p.requires_grad ]\n",
    "            hv = hvp(loss, params, h_estimate) \n",
    "            h_estimate = [\n",
    "                _v + (1 - damp) * _h_e - _hv / scale\n",
    "                for _v, _h_e, _hv in zip(v, h_estimate, hv)]\n",
    "            break\n",
    "    return h_estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ -81.2594, -125.6682, -170.0769, -205.2730],\n",
       "         [  45.3421,   64.4313,   83.5204,   96.0463],\n",
       "         [  48.8245,   76.4019,  103.9794,  126.2451]], grad_fn=<SubBackward0>),\n",
       " tensor([-44.4088,  19.0891,  27.5775], grad_fn=<SubBackward0>),\n",
       " tensor([[ 437.3736,   44.4160, -313.7211],\n",
       "         [ 498.6012,   91.0806, -363.9464]], grad_fn=<SubBackward0>),\n",
       " tensor([29.4857, 17.3848], grad_fn=<SubBackward0>)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 因为内层for循环一致算同一个点，所以当迭代次数增多，误差会很大\n",
    "# 正确的做法是随机从训练集取一定数量的点\n",
    "s_test(model, test_z, test_t, train_set, recursion_depth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step3: 合并所有的结果 计算每一个样本点对test_z的影响值\n",
    "def calc_influence_function(model, train_set, test_z, test_t):\n",
    "    \"\"\"Calculates the influence function\n",
    "\n",
    "    Returns:\n",
    "        influence: list of float, influences of all training data samples\n",
    "            for one test sample\n",
    "        harmful: list of float, influences sorted by harmfulness\n",
    "        helpful: list of float, influences sorted by helpfulness.\n",
    "    \"\"\"\n",
    "    train_dataset_size = len(train_set)\n",
    "    influences = []\n",
    "    \n",
    "    # list of torch tensor, containing the gradients from model parameters to loss\n",
    "    grad_z_vecs = [grad_z(model, z, t) for z, t in train_set] \n",
    "    # list of torch tensor, contains s_test vectors\n",
    "    e_s_test = s_test(model, test_z, test_t, train_set, recursion_depth=1)\n",
    "    \n",
    "    # 对第 i 个样本\n",
    "    for i in range(train_dataset_size):\n",
    "        influence_i = -sum(\n",
    "            [\n",
    "                torch.sum(k * j).data.cpu().numpy()\n",
    "                for k, j in zip(grad_z_vecs[i], e_s_test)\n",
    "            ]) / train_dataset_size\n",
    "        influences.append(influence_i)\n",
    "\n",
    "    harmful = np.argsort(influences)\n",
    "    helpful = harmful[::-1]\n",
    "\n",
    "    return influences, harmful.tolist(), helpful.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ -81.2594, -125.6682, -170.0769, -205.2730],\n",
      "        [  45.3421,   64.4313,   83.5204,   96.0463],\n",
      "        [  48.8245,   76.4019,  103.9794,  126.2451]], grad_fn=<SubBackward0>), tensor([-44.4088,  19.0891,  27.5775], grad_fn=<SubBackward0>), tensor([[ 437.3736,   44.4160, -313.7211],\n",
      "        [ 498.6012,   91.0806, -363.9464]], grad_fn=<SubBackward0>), tensor([29.4857, 17.3848], grad_fn=<SubBackward0>)]\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([14395.082336425781, 47184.150970458984], [0, 1], [1, 0])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_influence_function(model, train_set, test_z, test_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step1: 计算V第三项\n",
    "def grad_z2(model, z, t):\n",
    "    \"\"\" Calculates the gradient z. One grad_z should be computed for each\n",
    "    training sample.\n",
    "    \n",
    "    Arguments:\n",
    "        z: torch tensor, training data points\n",
    "            e.g. an image sample (batch_size, 3, 256, 256)\n",
    "        t: torch tensor, training data labels\n",
    "        model: torch NN, model used to evaluate the dataset\n",
    "    Returns:\n",
    "        grad_z: list of torch tensor, containing the gradients\n",
    "            from model parameters to loss\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    y = model(z)\n",
    "    loss = loss_fn(y, t)\n",
    "    grads = torch.autograd.grad(loss, model.parameters(), create_graph=True)\n",
    "    grads = torch.cat([x.flatten() for x in grads], dim=0)\n",
    "\n",
    "    return grads\n",
    "\n",
    "# grad = grad_z2(model, train_z, train_t)\n",
    "# grad\n",
    "# Step2: 计算前两项，也就是 s_test=v^T H^{-1}\n",
    "# 辅助函数：快速计算 Hv，其中 H 是 Hessian 矩阵\n",
    "def hvp2(y, model, v): # 计算 y 对 w 的二阶导 H，返回 Hv\n",
    "    \"\"\"Multiply the Hessians of y and w by v.\n",
    "    Uses a backprop-like approach to compute the product between the Hessian\n",
    "    and another vector efficiently, which even works for large Hessians.\n",
    "    Example: if: y = 0.5 * w^T A x then hvp(y, w, v) returns and expression\n",
    "    which evaluates to the same values as (A + A.t) v.\n",
    "\n",
    "    Arguments:\n",
    "        y: scalar/tensor, for example the output of the loss function\n",
    "        w: list of torch tensors, tensors over which the Hessian\n",
    "            should be constructed\n",
    "        v: list of torch tensors, same shape as w,\n",
    "            will be multiplied with the Hessian\n",
    "\n",
    "    Returns:\n",
    "        return_grads: list of torch tensors, contains product of Hessian and v.\n",
    "\n",
    "    \"\"\"\n",
    "    # First backprop\n",
    "    first_grads = torch.autograd.grad(y, model.parameters(), retain_graph=True, create_graph=True)\n",
    "    first_grads = torch.cat([x.flatten() for x in first_grads], dim=0)\n",
    "    # Elementwise products\n",
    "    elemwise_products = 0\n",
    "    for grad_elem, v_elem in zip(first_grads, v):\n",
    "        elemwise_products += torch.sum(grad_elem * v_elem)\n",
    "    \n",
    "    \n",
    "    # Second backprop\n",
    "    return_grads = torch.autograd.grad(elemwise_products, model.parameters(), create_graph=True)\n",
    "    return_grads = torch.cat([x.flatten() for x in return_grads], dim=0)\n",
    "    return return_grads\n",
    "\n",
    "\n",
    "def s_test2(model, test_z, test_t, train_loader, damp=0.01, scale=25.0,\n",
    "       recursion_depth=5000):\n",
    "    \"\"\"s_test can be precomputed for each test point of interest, and then\n",
    "    multiplied with grad_z to get the desired value for each training point.\n",
    "    Here, strochastic estimation is used to calculate s_test. s_test is the\n",
    "    Inverse Hessian Vector Product.\n",
    "\n",
    "    Arguments:\n",
    "        test_z: torch tensor, test data points, such as test images\n",
    "        test_t: torch tensor, contains all test data labels\n",
    "        model: torch NN, model used to evaluate the dataset\n",
    "        train_loader: torch Dataloader, can load the training dataset\n",
    "        damp: float, dampening factor\n",
    "        scale: float, scaling factor\n",
    "        recursion_depth: int, number of iterations aka recursion depth\n",
    "            should be enough so that the value stabilises.\n",
    "\n",
    "    Returns:\n",
    "        h_estimate: list of torch tensors, s_test\n",
    "    \"\"\"\n",
    "    v = grad_z2(model, test_z, test_t)\n",
    "    h_estimate = v.clone() # 不能再 .detache() !!!\n",
    "    for i in range(recursion_depth):\n",
    "        for z, t in train_loader:\n",
    "            y = model(z)\n",
    "            loss = loss_fn(y, t)\n",
    "            hv = hvp2(loss, model, h_estimate) \n",
    "            h_estimate = [\n",
    "                _v + (1 - damp) * _h_e - _hv / scale\n",
    "                for _v, _h_e, _hv in zip(v, h_estimate, hv)]\n",
    "            break\n",
    "    return h_estimate\n",
    "\n",
    "# Step3: 合并所有的结果 计算每一个样本点对test_z的影响值\n",
    "def calc_influence_function2(model, train_set, test_z, test_t):\n",
    "    \"\"\"Calculates the influence function\n",
    "\n",
    "    Arguments:\n",
    "        grad_z_vecs: list of torch tensor, containing the gradients\n",
    "            from model parameters to loss\n",
    "        e_s_test: list of torch tensor, contains s_test vectors\n",
    "\n",
    "    Returns:\n",
    "        influence: list of float, influences of all training data samples\n",
    "            for one test sample\n",
    "        harmful: list of float, influences sorted by harmfulness\n",
    "        helpful: list of float, influences sorted by helpfulness.\n",
    "    \"\"\"\n",
    "    train_dataset_size = len(train_set)\n",
    "    influences = []\n",
    "    \n",
    "    grad_z_vecs = [grad_z2(model, z, t) for z, t in train_set] \n",
    "    e_s_test = s_test2(model, test_z, test_t, train_set, recursion_depth=1)\n",
    "    \n",
    "    \n",
    "#     grad_z_vecs = torch.cat([x.flatten() for x in grad_z_vecs], dim=0)\n",
    "#     e_s_test = torch.cat([x.flatten() for x in e_s_test], dim=0)\n",
    "    \n",
    "    # 对第 i 个样本\n",
    "    for i in range(train_dataset_size):\n",
    "        influence_i = -sum(\n",
    "            [\n",
    "                torch.sum(k * j).data.cpu().numpy()\n",
    "                for k, j in zip(grad_z_vecs[i], e_s_test)\n",
    "            ]) / train_dataset_size\n",
    "        influences.append(influence_i)\n",
    "\n",
    "    harmful = np.argsort(influences)\n",
    "    helpful = harmful[::-1]\n",
    "\n",
    "    return influences, harmful.tolist(), helpful.tolist()\n",
    "\n",
    "calc_influence_function2(model, train_set, test_z, test_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([14395.081895828247, 47184.14908027649], [0, 1], [1, 0])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
